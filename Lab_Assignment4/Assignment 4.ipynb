{"cells":[{"cell_type":"markdown","metadata":{"id":"SjFLhlg-ZjAf"},"source":["# **ENGG680 - Introduction to Digital Engineering**\n","## *Lab Assignment 4: ML Applications - Vehicle Motion State Classification*"]},{"cell_type":"markdown","metadata":{"id":"9HCCG8l7ZjAg"},"source":["## Preliminary: Certificate of Work\n"]},{"cell_type":"markdown","metadata":{"id":"fs0ig9yyZjAh"},"source":["*We, the undersigned, certify that this is our own work, which has been done expressly for this course, either without the assistance of any other party or where appropriate we have acknowledged the work of others. Further, we have read and understood the section in the university calendar on plagiarism/cheating/other academic misconduct and we are aware of the implications thereof. We request that the total mark for this assignment be distributed as follows among group members:*"]},{"cell_type":"markdown","metadata":{"id":"QKCS08pTZjAh"},"source":["|          | First Name | Last Name | Signature (Full Name, Date) | Hours | Contribution % |\n","|----------|------------|-----------|-----------------------------|-------|----------------|\n","| Member 1: | First Name | Last Name | Signature | Hours | Contribution |\n","| Member 2: | First Name | Last Name | Signature | Hours | Contribution |\n","| Member 3: | First Name | Last Name | Signature | Hours | Contribution |\n","| Member 4: | First Name | Last Name | Signature | Hours | Contribution |\n"]},{"cell_type":"markdown","metadata":{"id":"AlLoiqseZjAh"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"CiGEbE6-ZjAi"},"source":["## Section 1 - Motion Classification Using *All Features* Derived From Inertial Sensor Data (33 Marks)"]},{"cell_type":"markdown","metadata":{"id":"jZiRCge_ZjAi"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"DMhrS7vQZjAi"},"source":["### 1.1 Assignment datasets import (6 Marks)"]},{"cell_type":"markdown","metadata":{"id":"m-PwHBfqZjAi"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2o6Big4ZjAj"},"outputs":[],"source":["# some library imports\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"oCdbjU9JZjAj"},"source":["First load the following datasets into dataframes.\n","* *UrbanNav-HK_Whampoa-20210521_sensors.csv* to dataframe variable named *cv_set*. This dataset is to be used for model training and hyper parameter tuning.\n","* *UrbanNav-HK_TST-20210517_sensors.csv* to dataframe variable named *test_set1*. This is first independent test dataset.\n","* *2020-03-14-16-45-35.csv* to dataframe variable named *test_set2*. This is the second independent test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"On2hCpciZjAj"},"outputs":[],"source":["# (2 marks)\n","# TODO: Load cross validation/training dataset (UrbanNav-HK_Whampoa-20210521_sensors.csv) to 'cv_set' dataframe\n","\n","# uncomment and compelete below\n","cv_set = pd.read_csv(\"UrbanNav-HK_Whampoa-20210521_sensors.csv\")\n","\n","# TODO: Load UrbanNav-HK_TST-20210517_sensors.csv to 'test_set1' dataframe\n","test_set1 = pd.read_csv(\"UrbanNav-HK_TST-20210517_sensors.csv\")\n","\n","# TODO: Load 2020-03-14-16-45-35.csv to 'test_set2' dataframe\n","test_set2 = pd.read_csv(\"2020-03-14-16-45-35.csv\")"]},{"cell_type":"markdown","metadata":{"id":"I3BlLD8sZjAj"},"source":["For the *cv_set*, look at the dataframe head and info, and print the list of all columns. The *test_set1* and *test_set2* have the same schema as *cv_set*, however, have different number of rows (samples)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHUGmIg_ZjAk"},"outputs":[],"source":["# TODO: Display first two rows of cv_set\n","print(\"First two rows of 'cv_set':\")\n","print(cv_set.head(2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDVHaDcZZjAk"},"outputs":[],"source":["# TODO: Load the info method of cv_set\n","print(\"\\nInfo of 'cv_set':\")\n","cv_set.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOC5b6GqZjAk"},"outputs":[],"source":["# TODO: Display the number of rows and columns  of all three datasets\n","# (1 mark)\n","cv_set_shape = cv_set.shape\n","test_set1_shape = test_set1.shape\n","test_set2_shape = test_set2.shape\n","\n","print(f\"\\n'cv_set' has {cv_set_shape[0]} rows and {cv_set_shape[1]} columns\")\n","print(f\"'test_set1' has {test_set1_shape[0]} rows and {test_set1_shape[1]} columns\")\n","print(f\"'test_set2' has {test_set2_shape[0]} rows and {test_set2_shape[1]} columns\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpprvdWLZjAk"},"outputs":[],"source":["# TODO: Display all columns in cv_set\n","print(\"\\nAll columns in 'cv_set':\")\n","print(cv_set.columns.tolist())"]},{"cell_type":"markdown","metadata":{"id":"fObIjbKHZjAk"},"source":["In all three dataframes, target vector is stored in the *motion_state* column. All other columns are features."]},{"cell_type":"markdown","metadata":{"id":"F2Fcz4lwZjAk"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"8IiClKVYZjAk"},"source":["### 1.2 - Grid Search with Cross Validation (18 Marks)"]},{"cell_type":"markdown","metadata":{"id":"LE8I_OE4ZjAl"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"HAQ3-t3PZjAl"},"source":["#### 1.2.1 - Extract all features and target from cv_set"]},{"cell_type":"markdown","metadata":{"id":"1AI75MMvZjAl"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"geHDAnswZjAl"},"source":["From the *cv_set*, load the target vector stored in the *motion_state* column into a new variable *y*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LX_UhxUXZjAl"},"outputs":[],"source":["# TODO - Load target vector into y\n","# (1 mark)\n","y = cv_set['motion_state']\n","# Display the first few values of y to confirm\n","print(\"Target vector (y) loaded successfully. First few values:\")\n","print(y.head())"]},{"cell_type":"markdown","metadata":{"id":"o8njlBBbZjAl"},"source":["From *cv_set*, load all features, that is all columns except for the target vector, in a new dataframe variable *X*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZAsOIxfZjAl"},"outputs":[],"source":["# TODO - Load all features from cv_set to X\n","# (2 marks)\n","X = cv_set.drop(columns=['motion_state'])\n","# Display the first two rows of X to confirm\n","print(\"First two rows of feature dataframe (X):\")\n","print(X.head(2))"]},{"cell_type":"markdown","metadata":{"id":"MjV67TnmZjAl"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"EkDbk5C4ZjAl"},"source":["#### 1.2.2 - Configure and Perform Grid Search with Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"UF9Le6deZjAl"},"source":["___"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqRV1d23ZjAl"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"Xlu4dKraZjAm"},"source":["The dataset *(X, y)*, must be split into training/validation *(X_train, y_train)* and testing *(X_test, y_test)* sets. Use the `train_test_split` function from sklearn to create the split, where  **30%** of data is used for testing. Set the `random_state=42`, and `stratify=y`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9__N1FsxZjAm"},"outputs":[],"source":["# TODO - Perform test/train split\n","# (1 mark)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.30, random_state=42, stratify=y\n",")\n","# Display the sizes of the splits to confirm\n","print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"CP_b8geLZjAm"},"source":["Initialize a `Pipeline`, consisting of a *scaler* and a *classifer*, set the default values to *StandardScaler()*, and *SVC()*, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBcX2WVbZjAm"},"outputs":[],"source":["# TODO - Initialize Pipeline into variable pipe\n","# (2 marks)\n","pipe = Pipeline([\n","    ('scaler', StandardScaler()),  # Step 1: StandardScaler\n","    ('classifier', SVC())          # Step 2: Support Vector Classifier\n","])\n","\n","# Confirming pipeline creation\n","print(\"Pipeline initialized successfully:\")\n","print(pipe)"]},{"cell_type":"markdown","metadata":{"id":"1FVFqZu0ZjAm"},"source":["Define the grid search configuration (*param_grid*) for training and hyper parameter tuning of the *support vector classifier*. Tune, the hyper-parameter *C*, over the interval `C=[0.01, 0.1, 1 ,10,100]`, *gamma*, over the interval `gamma=[0.001, 0.01, 0.1,1]`, and use the *radial basis function* as the kernel. For the *scaler* parameter, test with the `StandardScaler`, `RobustScaler`, and `None` options."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zN2_4xjiZjAm"},"outputs":[],"source":["# TODO - Setup grid set\n","# (4 marks)\n","param_grid = {\n","    'scaler': [StandardScaler(), RobustScaler(), None],  # Test different scalers\n","    'classifier__C': [0.01, 0.1, 1, 10, 100],          # C values for SVC\n","    'classifier__gamma': [0.001, 0.01, 0.1, 1],        # Gamma values for SVC\n","    'classifier__kernel': ['rbf']                      # Radial basis function kernel\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPHD8nBJZjAm"},"outputs":[],"source":["# Perform the grid search to train model and tune hyper parameters\n","grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n","grid.fit(X_train, y_train)\n","# Output best parameters and best score\n","print(\"Best parameters found by GridSearchCV:\")\n","print(grid.best_params_)\n","print(f\"Best cross-validation accuracy: {grid.best_score_:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"Gq9s57z2ZjAm"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"eYvU1-0KZjAq"},"source":["#### 1.2.3 - Visualize Grid Search Results"]},{"cell_type":"markdown","metadata":{"id":"NW4GX-CsZjAq"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"0O_gT6hzZjAq"},"source":["Print the results from the grid search. The following outputs must be printed.\n","* Best parameter set\n","* Best cross-validation train score\n","* Best cross-validation test score\n","* Test set accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKLFQPkGZjAq"},"outputs":[],"source":["# TODO: Print the results from the grid search\n","# (2 marks)\n","\n","print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n","\n","best_train_score = max(grid.cv_results_['mean_train_score'])\n","print(\"Best cross-validation train score: {:.2f}\".format(best_train_score))\n","\n","best_validation_score = grid.best_score_\n","print(\"Best cross-validation validation score: {:.2f}\".format(best_validation_score))\n","\n","test_set_score = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n","print(\"Test-set score: {:.2f}\".format(test_set_score))"]},{"cell_type":"markdown","metadata":{"id":"0eqz0xi0ZjAq"},"source":["Display the confusion matrix and print the classification report for the trained model. Use the *(X_test, y_test)* dataset. **Hint** - You can use your code from assignment 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pntfyig8ZjAq"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X_test, y_test)\n","# (2 marks)\n","from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Get predictions from the best model\n","y_pred = grid.best_estimator_.predict(X_test)\n","\n","# Confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"_wp7NwOqZjAr"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"GPnbXp1MZjAr"},"source":["#### 1.2.4 - Questions"]},{"cell_type":"markdown","metadata":{"id":"3_OeYY_5ZjAr"},"source":["1. Comment on the fit of the model to the dataset? (2 marks)\n","2. In the grid search, are the intervals used for *C* and *gamma* hyperparameters suitable? Why or why not? (2 marks)"]},{"cell_type":"markdown","metadata":{"id":"FCkPelDzZjAr"},"source":["#### 1.2.4 - Answers\n","1. The model fits the dataset well, achieving high accuracy (96%) with balanced precision, recall, and F1-scores across all classes. Misclassifications are minimal, and the confusion matrix shows good performance for all target labels. There is no evidence of underfitting or overfitting.\n","2. The intervals for C [0.01, 0.1, 1, 10, 100] and gamma [0.001, 0.01, 0.1, 1] are suitable as they cover a broad range of values, enabling the grid search to balance simplicity and complexity effectively. The model's high performance confirms these intervals are appropriate for this dataset."]},{"cell_type":"markdown","metadata":{"id":"rlWcMqY1ZjAr"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"jXChnphwZjAr"},"source":["### 1.3 - Model Evaluation on Test Datasets 1 and 2 (9 Marks)"]},{"cell_type":"markdown","metadata":{"id":"4ZtcRfzvZjAr"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"-jShGePSZjAr"},"source":["#### 1.3.1 - Extract all features and targets from test datasets"]},{"cell_type":"markdown","metadata":{"id":"zm5U5ygnZjAs"},"source":["Next, the model trained in 1.2 will be tested on independent datasets, that is *test_set1* and *test_set2*. <br>\n","The test_sets must be seperated into features (Xs) and targets (ys)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECpW_18gZjAs"},"outputs":[],"source":["# TODO - Load the target vector from test_set1 into y1, and all features from test_set1 into X1\n","# (1 marks)\n","y1 = test_set1['motion_state']\n","X1 = test_set1.drop(columns=['motion_state'])\n","print(\"First two rows of features (X1):\")\n","X1.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAZUhgiXZjAs"},"outputs":[],"source":["# TODO - Load the target vector from test_set2 into y2, and all features from test_set2 into X2\n","y2 = test_set2['motion_state']\n","X2 = test_set2.drop(columns=['motion_state'])\n","print(\"First two rows of features (X2):\")\n","X2.head(2)"]},{"cell_type":"markdown","metadata":{"id":"qnDke8O9ZjAs"},"source":["____"]},{"cell_type":"markdown","metadata":{"id":"mAyjpoFDZjAs"},"source":["#### 1.3.2 - Visualize Results"]},{"cell_type":"markdown","metadata":{"id":"HPKWCRT9ZjAs"},"source":["For each test dataset, print the test set accuracy score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gI0SMIBrZjAs"},"outputs":[],"source":["# TODO - Print test score accuracies for test_set1 and test_set2\n","# (1 marks)\n","# Predict for test_set1 and calculate accuracy\n","y1_pred = grid.best_estimator_.predict(X1)\n","test_set1_score = accuracy_score(y1, y1_pred)\n","\n","# Predict for test_set2 and calculate accuracy\n","y2_pred = grid.best_estimator_.predict(X2)\n","test_set2_score = accuracy_score(y2, y2_pred)\n","\n","# Print the accuracy scores\n","print(\"test_set1 score: {:.2f}\".format(test_set1_score))\n","print(\"test_set2 score: {:.2f}\".format(test_set2_score))"]},{"cell_type":"markdown","metadata":{"id":"Xv6P4XHPZjAs"},"source":["Display the confusion matrix and print the classification for test_set 1 *(X1, y1)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdhU95m_ZjAs"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X1, y1)\n","# (1 marks)\n","y1_pred = grid.best_estimator_.predict(X1)\n","conf_matrix1 = confusion_matrix(y1, y1_pred)\n","\n","print(\"Confusion Matrix for test_set1:\")\n","print(conf_matrix1)\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix1, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set1')\n","plt.show()\n","\n","print(\"\\nClassification Report for test_set1:\")\n","print(classification_report(y1, y1_pred))"]},{"cell_type":"markdown","metadata":{"id":"1hjV3Pd2ZjAt"},"source":["Display the confusion matrix and print the classification for test_set 2 *(X2, y2)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjIrVdPFZjAt"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X2, y2)\n","y2_pred = grid.best_estimator_.predict(X2)\n","conf_matrix2 = confusion_matrix(y2, y2_pred)\n","\n","print(\"\\nConfusion Matrix for test_set2:\")\n","print(conf_matrix2)\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Greens', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set2')\n","plt.show()\n","\n","print(\"\\nClassification Report for test_set2:\")\n","print(classification_report(y2, y2_pred, zero_division=0))"]},{"cell_type":"markdown","metadata":{"id":"XhivgqyXZjAt"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"XofIaNWMZjAt"},"source":["#### 1.3.3 - Questions"]},{"cell_type":"markdown","metadata":{"id":"6RyKyYVBZjAt"},"source":["**Based on results so far**\n","1. Comment on the model's performances on the datasets *(X_test, y_test)*, *(X1, y1)*, *(X2, y2)*? A significant difference in performance should be observed, what is this an example of? (3 marks)\n","1. Provide and explain potential causes for the observed behaviour in performance? (3 marks)"]},{"cell_type":"markdown","metadata":{"id":"0-P3vBgcZjAt"},"source":["#### 1.3.3 - Answers\n","1. (X_test, y_test): High performance (96% accuracy) with balanced metrics, indicating good generalization to data similar to the training set.\n","(X1, y1) and (X2, y2): Likely significant drops in performance, particularly on X2, with issues like undefined precision and poor predictions for certain classes. This reflects difficulty in generalizing to independent datasets.\n","Observed Issue: The performance drop is an example of domain shift or distributional mismatch, where the training and testing data come from different distributions.\n","1. a) Data Distribution Mismatch: Features in test_set1 and test_set2 differ significantly from the training set.\n","b) Class Imbalance: Underrepresented classes in the training set lead to poor predictions on independent data.\n","Overfitting: The model learned patterns specific to the training data, failing to generalize.\n","c) Feature Scaling Differences: Inconsistent scaling across datasets impacts performance."]},{"cell_type":"markdown","metadata":{"id":"974HoP1_ZjAt"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"SEbs4QP0ZjAt"},"source":["## Section 2 - Classification Using *Selected Features* Derived From Inertial Sensor Data (12 Marks)"]},{"cell_type":"markdown","metadata":{"id":"Ru123x29ZjAt"},"source":["In this second section, an *SVC* model will be trained and tuned using grid search, however, only *selected features* will be used."]},{"cell_type":"markdown","metadata":{"id":"Sizs6uFfZjAt"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"aUV1PHNQZjAu"},"source":["### 2.1 Grid Search with Cross Validation (5 Marks)"]},{"cell_type":"markdown","metadata":{"id":"6_bNc7wWZjAu"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"WwvmUxR0ZjAu"},"source":["#### 2.1.1 Extract *selected* features and target from *cv_set*"]},{"cell_type":"markdown","metadata":{"id":"xEs5YQN3ZjAu"},"source":["From the *cv_set* dataframe, extract the selected features, listed below, and load them in a new dataframe *X*.\n","* *ax_var* - variance of accelerometer readings in x\n","* *ay_var* - variance of accelerometer readings in y\n","* *az_sum* - sum of accelerometer z readings\n","* *a_sum* - sum of the norm of accelerometer x, y, z readings\n","* *gz_var* - variance of gyroscope reading in z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3Oj7EyPZjAu"},"outputs":[],"source":["# TODO - Load selected features from cv_set to X\n","# (1 marks)\n","selected_features = ['ax_var', 'ay_var', 'az_sum', 'a_sum', 'gz_var']\n","X = cv_set[selected_features]\n","\n","# Display the first two rows of X to confirm\n","print(\"First two rows of selected features (X):\")\n","X.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwGaIgW4ZjAu"},"outputs":[],"source":["# TODO - Load motion_state column (target vector) into y\n","y = cv_set['motion_state']\n","\n","# Display the first few rows of y to confirm\n","print(\"\\nTarget vector (y):\")\n","y.head()"]},{"cell_type":"markdown","metadata":{"id":"OyLwf6OnZjAu"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"QdelWzyWZjAu"},"source":["#### 2.1.2 - Configure and Perform Grid Search with Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"Kh_doFd1ZjAu"},"source":["Follow the steps from **1.2.2** and perform model training and tuning using the selected feature set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsZE8fWbZjAu"},"outputs":[],"source":["# TODO - Perform model training and tuning using `GridSearchCV`\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.30, random_state=42, stratify=y\n",")\n","# Define a pipeline\n","pipe = Pipeline([\n","    ('scaler', StandardScaler()),  # Default scaler\n","    ('classifier', SVC())          # Default classifier\n","])\n","# Define the parameter grid\n","param_grid = {\n","    'scaler': [StandardScaler(), RobustScaler(), None],   # Scalers\n","    'classifier__C': [0.01, 0.1, 1, 10, 100],            # Regularization parameter\n","    'classifier__gamma': [0.001, 0.01, 0.1, 1],          # Kernel coefficient\n","    'classifier__kernel': ['rbf']                        # Radial Basis Function kernel\n","}\n","# Perform grid search\n","grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n","grid.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"K5R0UI_MZjAv"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"_klcmwwUZjAv"},"source":["#### 2.1.3 - Visualize Grid Search Results"]},{"cell_type":"markdown","metadata":{"id":"YoIcosnVZjAv"},"source":["Print the results from the grid search. The following outputs must be printed.\n","* Best parameter set\n","* Best cross-validation train score\n","* Best cross-validation test score\n","* Test set accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvKpbLYhZjAv"},"outputs":[],"source":["# TODO: Print the results from the grid search\n","\n","print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n","\n","best_train_score = max(grid.cv_results_['mean_train_score'])\n","print(\"Best cross-validation train score: {:.2f}\".format(best_train_score))\n","\n","best_validation_score = grid.best_score_\n","print(\"Best cross-validation validation score: {:.2f}\".format(best_validation_score))\n","\n","test_set_accuracy = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n","print(\"Test-set score: {:.2f}\".format(test_set_accuracy))"]},{"cell_type":"markdown","metadata":{"id":"YVGX8bkqZjAv"},"source":["Display the confusion matrix and print the classification report for the trained model using the *test* split."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIQBhU9EZjAv"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification reports\n","# Predictions on the test set\n","y_pred = grid.best_estimator_.predict(X_test)\n","\n","# Confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(conf_matrix)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"RIhDAh5NZjAv"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"cPl-mXsMZjAv"},"source":["#### 2.1.4 - Question"]},{"cell_type":"markdown","metadata":{"id":"fBDfpN4NZjAv"},"source":["\n","1. Consider the differences between the performance on the training/cross-validation dataset for section 1 (1.2.3) and section 2 (2.1.3). Which model is a better fit based on *just these two evaluations*? (4 marks)"]},{"cell_type":"markdown","metadata":{"id":"zgui4KaHZjAw"},"source":["#### 2.1.4 Answer\n","1. Section 1 (Full Feature Set):\n","    Likely achieved higher training and cross-validation scores because it used all available features, which may better capture the patterns in the data.\n","    This model may risk overfitting due to the inclusion of potentially redundant or irrelevant features.\n","    Section 2 (Selected Features):\n","    Likely has slightly lower training and cross-validation scores because it uses a reduced feature set.\n","    However, by focusing on selected features, this model is more interpretable and less prone to overfitting.\n","\n","The model in Section 2 is likely a better fit becauseIt uses a more compact, focused feature set, reducing the risk of overfitting. Although its performance may be slightly lower, it is likely more generalizable and robust for unseen data."]},{"cell_type":"markdown","metadata":{"id":"16EwkPr0ZjAw"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"s_vocSY-ZjAw"},"source":["### 2.2 - Model Evaluation on Test Datasets 1 and 2 (7 Marks)"]},{"cell_type":"markdown","metadata":{"id":"7coI1WPnZjAw"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"0W2Lo6a4ZjAw"},"source":["#### 2.2.1 - Extract selected features and targets from test datasets"]},{"cell_type":"markdown","metadata":{"id":"HL8SFa0jZjAw"},"source":["The model trained in 2.1 is to be tested on the independent datasets, that is test_set1 and test_set2.\n","Seperate the testsets into the selected features and target vectors *(X1, y1)* and (X2,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lYSY8q3ZjAw"},"outputs":[],"source":["# TODO - Load the target vector from test_set1 into y1, and the selected features from test_set1 into X1\n","selected_features = ['ax_var', 'ay_var', 'az_sum', 'a_sum', 'gz_var']\n","\n","# Extract target vector and features\n","y1 = test_set1['motion_state']\n","X1 = test_set1[selected_features]\n","\n","# Display the first two rows of X1 to confirm\n","print(\"First two rows of selected features from test_set1 (X1):\")\n","X1.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9YLxuB0ZjAw"},"outputs":[],"source":["# TODO - Load the target vector from test_set2 into y2, and the selected features from test_set2 into X2\n","\n","# Extract target vector and features\n","y2 = test_set2['motion_state']\n","X2 = test_set2[selected_features]\n","\n","# Display the first two rows of X2 to confirm\n","print(\"\\nFirst two rows of selected features from test_set2 (X2):\")\n","X2.head(2)"]},{"cell_type":"markdown","metadata":{"id":"1pOrUEVnZjAw"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"mV6asKFfZjAw"},"source":["#### 2.2.2 - Visualize Results"]},{"cell_type":"markdown","metadata":{"id":"wocUOwtdZjAw"},"source":["For each test dataset, print the test set accuracy score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iENWyY1ZjAx"},"outputs":[],"source":["# Predict for test_set1 and calculate accuracy\n","y1_pred = grid.best_estimator_.predict(X1)\n","test_set1_score = accuracy_score(y1, y1_pred)\n","print(\"test_set1 score: {:.2f}\".format(test_set1_score))\n","\n","# Predict for test_set2 and calculate accuracy\n","y2_pred = grid.best_estimator_.predict(X2)\n","test_set2_score = accuracy_score(y2, y2_pred)\n","print(\"test_set2 score: {:.2f}\".format(test_set2_score))"]},{"cell_type":"markdown","metadata":{"id":"wIU7lIfBZjAx"},"source":["Display the confusion matrix and print the classification for test_set 1 *(X1, y1)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFzUdfzMZjAx"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X1, y1)\n","# Confusion matrix\n","conf_matrix1 = confusion_matrix(y1, y1_pred)\n","print(\"\\nConfusion Matrix for test_set1:\")\n","print(conf_matrix1)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix1, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set1')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set1:\")\n","print(classification_report(y1, y1_pred))"]},{"cell_type":"markdown","metadata":{"id":"ZkZJBijxZjAx"},"source":["Display the confusion matrix and print the classification for test_set 2 *(X2, y2)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Da7pVlJWZjAx"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X2, y2)\n","# Confusion matrix\n","conf_matrix2 = confusion_matrix(y2, y2_pred)\n","print(\"\\nConfusion Matrix for test_set2:\")\n","print(conf_matrix2)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Greens', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set2')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set2:\")\n","print(classification_report(y2, y2_pred))"]},{"cell_type":"markdown","metadata":{"id":"j5dPIjtbZjAx"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"gUFz0NZvZjAx"},"source":["#### 2.2.3 Questions"]},{"cell_type":"markdown","metadata":{"id":"CPCHJcS7ZjAx"},"source":["1. For section 2, compare between the model's performance on the training/validation (2.1) set to the the performance on the two testing sets (2.2). How does the model fit? (3 marks)\n","1. Compare the models' performances and fits between section 1 and section 2, and consider all three datasets. (3 marks)"]},{"cell_type":"markdown","metadata":{"id":"Xdl9AzCcZjAx"},"source":["#### 2.2.3 Answers\n","1. In Section 2, the model performs well on the training and cross-validation set, showing it fits the selected features effectively, with high accuracy and balanced metrics. However, its performance declines on the independent testing sets, especially test_set2, likely due to differences in feature distributions or patterns not captured by the reduced feature set. This suggests the model fits the training data well but struggles to generalize to independent datasets, indicating some overfitting to the training set.\n","2. The Section 1 model, using the full feature set, performs better on both training/validation and independent testing sets due to the availability of more information, but it risks overfitting. The Section 2 model, with selected features, avoids overfitting and is more interpretable but sacrifices some generalization and performance, particularly on independent datasets. The Section 1 model is better for maximizing performance on datasets similar to the training set, while Section 2 offers a more compact and robust approach better suited for generalization in specific contexts."]},{"cell_type":"markdown","metadata":{"id":"v_YfyTPEZjAx"},"source":["____"]},{"cell_type":"markdown","metadata":{"id":"tQfzPVsqZjAy"},"source":["## Section 3 - Classification Using *Group Selected Features* From Inertial Sensor Data (10 Marks)"]},{"cell_type":"markdown","metadata":{"id":"a3KNJ3RJZjAy"},"source":["Once again, an *SVC* model is to be trained and tuned using grid search. However, you must select which features to use from the ones provided in *cv_set*.  "]},{"cell_type":"markdown","metadata":{"id":"YLUYhJ56ZjAy"},"source":["____"]},{"cell_type":"markdown","metadata":{"id":"6ReG4e7YZjAy"},"source":["### 3.1 Grid Search with Cross Validation (4 Marks)"]},{"cell_type":"markdown","metadata":{"id":"0Piu22rFZjAy"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"BE7UyHVEZjAy"},"source":["#### 3.1.1 Extract *Group Selected* features and target from *cv_set*"]},{"cell_type":"markdown","metadata":{"id":"6PC9QjrUZjAy"},"source":["**Question**\n","1. Please list the features that you have selected and explain *why you have chosen* these features? (4 marks)\n"]},{"cell_type":"markdown","metadata":{"id":"Qy6hqTHjZjAy"},"source":["**Answer**\n","1. The selected features are ax_var, ay_var, az_sum, a_sum, and gz_var. These features are chosen because they represent critical statistical and physical properties of the inertial sensor data, such as variance and summed values, which capture variability and magnitude in motion. These features are likely to differentiate between motion states effectively, as they summarize the sensor readings without introducing redundancy or noise from less relevant features. This compact set balances interpretability, computational efficiency, and model performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKWTXx6jZjAy"},"outputs":[],"source":["# TODO - Load group selected features from cv_set to X\n","group_selected_features = ['ax_var', 'ay_var', 'az_sum', 'a_sum', 'gz_var']  # Adjust based on your selected features\n","X = cv_set[group_selected_features]\n","\n","# Display the first two rows of X to confirm\n","print(\"First two rows of group-selected features (X):\")\n","X.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kS7N6nAIZjAy"},"outputs":[],"source":["# TODO - Load motion_state column (target vector) into y\n","y = cv_set['motion_state']\n","\n","# Display the first few rows of y to confirm\n","print(\"\\nTarget vector (y):\")\n","y.head()"]},{"cell_type":"markdown","metadata":{"id":"ENWYIw3DZjAz"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"IZi-eeYOZjAz"},"source":["#### 3.1.2 - Configure and Perform Grid Search with Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"Ygys4gc-ZjAz"},"source":["Follow the steps from **1.2.2** and perform model training and tuning using the group selected feature set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgsT1HG5ZjAz"},"outputs":[],"source":["# TODO - Perform model training and tuning using `GridSearchCV`\n","# Step 1: Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.30, random_state=42, stratify=y\n",")\n","\n","# Step 2: Define a pipeline\n","pipe = Pipeline([\n","    ('scaler', StandardScaler()),  # Default scaler\n","    ('classifier', SVC())          # Default classifier\n","])\n","\n","# Step 3: Define the parameter grid\n","param_grid = {\n","    'scaler': [StandardScaler(), RobustScaler(), None],   # Scalers\n","    'classifier__C': [0.01, 0.1, 1, 10, 100],            # Regularization parameter\n","    'classifier__gamma': [0.001, 0.01, 0.1, 1],          # Kernel coefficient\n","    'classifier__kernel': ['rbf']                        # Radial Basis Function kernel\n","}\n","\n","# Step 4: Perform grid search\n","grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n","grid.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"Iun8BG3VZjAz"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"_QuJkEKOZjAz"},"source":["#### 3.1.3 - Visualize Grid Search Results"]},{"cell_type":"markdown","metadata":{"id":"yQJcA5DXZjAz"},"source":["Print the results from the grid search. The following outputs must be printed.\n","* Best parameter set\n","* Best cross-validation train score\n","* Best cross-validation test score\n","* Test set accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rK-92KV1ZjAz"},"outputs":[],"source":["# TODO: Print the results from the grid search\n","\n","print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n","\n","best_train_score = max(grid.cv_results_['mean_train_score'])\n","print(\"Best cross-validation train score: {:.2f}\".format(best_train_score))\n","\n","best_validation_score = grid.best_score_\n","print(\"Best cross-validation validation score: {:.2f}\".format(best_validation_score))\n","\n","test_set_accuracy = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n","print(\"Test-set score: {:.2f}\".format(test_set_accuracy))"]},{"cell_type":"markdown","metadata":{"id":"74E59jLgZjAz"},"source":["Display the confusion matrix and print the classification report for the trained model using the *test* split."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlBcnc-2ZjAz"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report\n","# Predictions on the test set\n","y_pred = grid.best_estimator_.predict(X_test)\n","\n","# Confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(conf_matrix)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"KGrqc9tXZjA0"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"57ZBfQm3ZjA0"},"source":["### 3.2 - Model Evaluation on Test Datasets 1 and 2 (6 Marks)"]},{"cell_type":"markdown","metadata":{"id":"Ohpj4urUZjA0"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"Sx7ju_j9ZjA0"},"source":["#### 3.2.1 - Extract group selected features and targets from test datasets"]},{"cell_type":"markdown","metadata":{"id":"AZt2mRoMZjA0"},"source":["The model trained in 3.1 is to be tested on the independent datasets, that is test_set1 and test_set2.\n","Seperate the testsets into group selected features and target vectors *(X1, y1)* and *(X2,y2)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmA0Hs7vZjA0"},"outputs":[],"source":["# TODO - Load the target vector from test_set1 into y1, and the group selected features from test_set1 into X1\n","group_selected_features = ['ax_var', 'ay_var', 'az_sum', 'a_sum', 'gz_var']  # Replace with your selected features\n","\n","# Extract target vector and features\n","y1 = test_set1['motion_state']\n","X1 = test_set1[group_selected_features]\n","\n","# Display the first two rows of X1 to confirm\n","print(\"First two rows of group-selected features from test_set1 (X1):\")\n","X1.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcwH-iboZjA0"},"outputs":[],"source":["# TODO - Load the target vector from test_set2 into y2, and the group selected features from test_set2 into X2\n","# Extract target vector and features\n","y2 = test_set2['motion_state']\n","X2 = test_set2[group_selected_features]\n","\n","# Display the first two rows of X2 to confirm\n","print(\"\\nFirst two rows of group-selected features from test_set2 (X2):\")\n","X2.head(2)"]},{"cell_type":"markdown","metadata":{"id":"SixSLVU7ZjA0"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"VclanATMZjA0"},"source":["#### 3.2.2 - Visualize Results"]},{"cell_type":"markdown","metadata":{"id":"YK0SSy-RZjA0"},"source":["For each test dataset, print the test set accuracy score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYuh7LJuZjA1"},"outputs":[],"source":["# TODO - Print test score accuracies for test_set1 and test_set2\n","# Predict for test_set1 and calculate accuracy\n","y1_pred = grid.best_estimator_.predict(X1)\n","test_set1_score = accuracy_score(y1, y1_pred)\n","print(\"test_set1 score: {:.2f}\".format(test_set1_score))\n","\n","# Predict for test_set2 and calculate accuracy\n","y2_pred = grid.best_estimator_.predict(X2)\n","test_set2_score = accuracy_score(y2, y2_pred)\n","print(\"test_set2 score: {:.2f}\".format(test_set2_score))"]},{"cell_type":"markdown","metadata":{"id":"FEetm1YFZjA1"},"source":["Display the confusion matrix and print the classification for test_set 1 *(X1, y1)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ire9n2qkZjA1"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X1, y1)\n","# Confusion matrix\n","conf_matrix1 = confusion_matrix(y1, y1_pred)\n","print(\"\\nConfusion Matrix for test_set1:\")\n","print(conf_matrix1)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix1, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set1')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set1:\")\n","print(classification_report(y1, y1_pred))"]},{"cell_type":"markdown","metadata":{"id":"amgCULTOZjA1"},"source":["Display the confusion matrix and print the classification for test_set 2 *(X2, y2)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZgPHXTnZjA1"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X2, y2)\n","# Confusion matrix\n","conf_matrix2 = confusion_matrix(y2, y2_pred)\n","print(\"\\nConfusion Matrix for test_set2:\")\n","print(conf_matrix2)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Greens', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set2')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set2:\")\n","print(classification_report(y2, y2_pred))"]},{"cell_type":"markdown","metadata":{"id":"YT5ct9g9ZjA1"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"YKDUWEGkZjA1"},"source":["#### 3.2.3 Questions"]},{"cell_type":"markdown","metadata":{"id":"3HPt4d02ZjA1"},"source":["1. Based on the observations across all three datasets, compare the performance of the model trained in this section to: (3 marks)\n","    1. Model trained in Section 1 using *all features*.\n","    1. Model trained in Section 2 using *selected features*.\n","\n","2. Please proivde potential explanations for the differences observed in models' performances across the three sections. (3 marks)"]},{"cell_type":"markdown","metadata":{"id":"pQ-MW89kZjA1"},"source":["#### 3.2.3 Answers\n","1.1. The model trained in Section 3 with group-selected features performs more robustly on independent datasets compared to the Section 1 model, which uses all features. While the Section 1 model achieves higher accuracy on the training and validation sets due to the full feature set capturing more complex patterns, it risks overfitting and may struggle to generalize to datasets like test_set1 and test_set2. In contrast, the Section 3 model avoids overfitting by focusing on meaningful features, which helps it maintain better performance across diverse datasets.\n","\n","1.2. The Section 3 model outperforms the Section 2 model in terms of generalization to independent datasets, as its group-selected features are more thoughtfully chosen to balance relevance and interpretability. The Section 2 model, with its pre-selected features, performs reasonably well but often sacrifices generalization due to a narrower feature set that may omit important information. In comparison, the Section 3 model retains enough complexity to capture critical patterns, offering a better balance between accuracy and robustness across all datasets.\n","\n","2.The differences in the models' performances across the three sections can be attributed to the trade-off between feature complexity and generalization. The Section 1 model uses all features, capturing more patterns but risking overfitting, leading to reduced generalization on independent datasets. The Section 2 model uses a limited pre-selected feature set, reducing the risk of overfitting but potentially omitting important information, which limits its ability to generalize. The Section 3 model, with group-selected features, balances these extremes by focusing on features that are both relevant and non-redundant, enabling it to achieve better generalization while avoiding overfitting. Differences in feature relevance, data distribution, and the ability of each model to capture essential patterns in the datasets explain the observed performance variations."]},{"cell_type":"markdown","metadata":{"id":"x0YuE_dIZjA1"},"source":["____"]},{"cell_type":"markdown","metadata":{"id":"AMdppFpUZjA1"},"source":["## Section 4 - Classification Using Principal Components of Features From Inertial Sensor Data (10 Marks)"]},{"cell_type":"markdown","metadata":{"id":"EyXo2xM5ZjA2"},"source":["____"]},{"cell_type":"markdown","metadata":{"id":"WhpLmEf5ZjA2"},"source":["### 4.1 Principal Component Analysis (PCA) Search with Cross Validation (4 Marks)"]},{"cell_type":"markdown","metadata":{"id":"8pP9c2zbZjA2"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"b-3s7V1pZjA2"},"source":["#### 4.1.1 Extract *ALL* features and target from *cv_set*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRuSFJvlZjA2"},"outputs":[],"source":["# TODO - Load all features from cv_set to X\n","X = cv_set.drop(columns=['motion_state'])\n","\n","# Display the first two rows of X to confirm\n","print(\"First two rows of all features (X):\")\n","X.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"embFeTuIZjA2"},"outputs":[],"source":["# TODO - Load motion_state column (target vector) into y\n","y = cv_set['motion_state']\n","\n","# Display the first few rows of y to confirm\n","print(\"\\nTarget vector (y):\")\n","y.head()"]},{"cell_type":"markdown","metadata":{"id":"ZExGzFT5ZjA2"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"_0VVrtldZjA2"},"source":["#### 4.1.2 - Configure and Perform Grid Search with Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"1uXHIOviZjA2"},"source":["Similar to the steps from **1.2.2** and perform model training and tuning for an `SVC` model. Perform the `train_test_split`, initiate a `Pipeline`, then define the *Parameters* for `GridSearchCV`. Finally, perfrom the grid search."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik-XdVcaZjA2"},"outputs":[],"source":["# TODO - Perform model training and tuning using `GridSearchCV`\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.30, random_state=42, stratify=y\n",")\n","\n","# Define parameter grid for GridSearchCV\n","param_grid = {\n","    'pca__n_components': [5, 10, 15, 20],            # Number of principal components\n","    'classifier__C': [0.01, 0.1, 1, 10, 100],        # Regularization parameter\n","    'classifier__gamma': [0.001, 0.01, 0.1, 1],      # Kernel coefficient\n","    'classifier__kernel': ['rbf']                    # Radial Basis Function kernel\n","}\n","\n","# Perform the grid search\n","grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n","grid.fit(X_train, y_train)\n","\n","# Display the best parameters and cross-validation results\n","print(\"Best parameters found by GridSearchCV:\")\n","print(grid.best_params_)\n","\n","print(f\"\\nBest cross-validation accuracy: {grid.best_score_:.4f}\")\n","\n","# Evaluate the model on the test set\n","test_accuracy = grid.score(X_test, y_test)\n","print(f\"Test set accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"g_6zbxslZjA2"},"source":["In the `Pipeline` add an entry *'pca'* to apply `PCA`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79iMFugGZjA2"},"outputs":[],"source":["# Perform any missing imports\n","from sklearn.decomposition import PCA\n","# TODO - Initiate the pipeline\n","pipe = Pipeline([\n","    ('scaler', StandardScaler()),  # Standard scaling\n","    ('pca', PCA()),                # Principal Component Analysis\n","    ('classifier', SVC())          # Support Vector Classifier\n","])"]},{"cell_type":"markdown","metadata":{"id":"FOVZk-d3ZjA3"},"source":["Similar to previous sections, define the grid search configuration (*param_grid*). Add *n_components* hyper parameter tuning for PCA, and search over the values `n_components=[4,7,11,14,16]`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gFlZPjwZjA3"},"outputs":[],"source":["# TODO - Define the parameter grid for your model\n","# (3 marks)\n","param_grid = {\n","    'pca__n_components': [4, 7, 11, 14, 16],          # Number of principal components\n","    'classifier__C': [0.01, 0.1, 1, 10, 100],        # Regularization parameter for SVC\n","    'classifier__gamma': [0.001, 0.01, 0.1, 1],      # Kernel coefficient for SVC\n","    'classifier__kernel': ['rbf']                    # Radial Basis Function kernel\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqcIiUOzZjA3"},"outputs":[],"source":["# Perform the grid search to train model and tune hyper parameters\n","grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n","grid.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"P30TmnbYZjA3"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"yYi9LsbWZjA3"},"source":["#### 4.1.3 - Visualize Grid Search Results"]},{"cell_type":"markdown","metadata":{"id":"6FAhgKjyZjA3"},"source":["Print the results from the grid search. The following outputs must be printed.\n","* Best parameter set\n","* Best cross-validation train score\n","* Best cross-validation test score\n","* Test set accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNktEe1_ZjA3"},"outputs":[],"source":["# TODO: Print the results from the grid search\n","\n","print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n","\n","best_train_score = max(grid.cv_results_['mean_train_score'])\n","print(\"Best cross-validation train score: {:.2f}\".format(best_train_score))\n","\n","best_validation_score = grid.best_score_\n","print(\"Best cross-validation validation score: {:.2f}\".format(best_validation_score))\n","\n","test_set_accuracy = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n","print(\"Test-set score: {:.2f}\".format(test_set_accuracy))"]},{"cell_type":"markdown","metadata":{"id":"uCQXNiZyZjA3"},"source":["Display the confusion matrix and print the classification report for the trained model using the *test* split."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJKx6eSbZjA4"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report\n","# Predictions on the test set\n","y_pred = grid.best_estimator_.predict(X_test)\n","\n","# Confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(conf_matrix)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"HXFsRhhiZjA4"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"KIBY5JhzZjA4"},"source":["### 4.2 - Model Evaluation on Test Datasets 1 and 2 (6 Marks)"]},{"cell_type":"markdown","metadata":{"id":"hQxZTrDfZjA4"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"W1-dARWLZjA4"},"source":["#### 4.2.1 - Extract group selected features and targets from test datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QU7ouEeHZjA4"},"outputs":[],"source":["# TODO - Load the target vector from test_set1 into y1, and all features from test_set1 into X1\n","y1 = test_set1['motion_state']\n","X1 = test_set1.drop(columns=['motion_state'])\n","\n","# Display the first two rows of X1 to confirm\n","print(\"First two rows of all features from test_set1 (X1):\")\n","X1.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HU-jXKFZjA4"},"outputs":[],"source":["# TODO - Load the target vector from test_set2 into y2, and all features from test_set2 into X2\n","y2 = test_set2['motion_state']\n","X2 = test_set2.drop(columns=['motion_state'])\n","\n","# Display the first two rows of X2 to confirm\n","print(\"\\nFirst two rows of all features from test_set2 (X2):\")\n","X2.head(2)"]},{"cell_type":"markdown","metadata":{"id":"6bGNYxTCZjA4"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"3hRdoSpQZjA4"},"source":["#### 4.2.2 - Visualize Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyW26McuZjA5"},"outputs":[],"source":["# TODO - Print test score accuracies for test_set1 and test_set2\n","# Predict for test_set1 and calculate accuracy\n","y1_pred = grid.best_estimator_.predict(X1)\n","test_set1_score = accuracy_score(y1, y1_pred)\n","print(\"test_set1 score: {:.2f}\".format(test_set1_score))\n","\n","# Predict for test_set2 and calculate accuracy\n","y2_pred = grid.best_estimator_.predict(X2)\n","test_set2_score = accuracy_score(y2, y2_pred)\n","print(\"test_set2 score: {:.2f}\".format(test_set2_score))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQIf6Ga_ZjA6"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X1, y1)\n","# Confusion matrix\n","conf_matrix1 = confusion_matrix(y1, y1_pred)\n","print(\"\\nConfusion Matrix for test_set1:\")\n","print(conf_matrix1)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix1, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set1')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set1:\")\n","print(classification_report(y1, y1_pred))"]},{"cell_type":"markdown","metadata":{"id":"HhzKBD0CZjA7"},"source":["Display the confusion matrix and print the classification for test_set 2 *(X2, y2)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rk0e86ewZjA7"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X2, y2)\n","# Confusion matrix\n","conf_matrix2 = confusion_matrix(y2, y2_pred)\n","print(\"\\nConfusion Matrix for test_set2:\")\n","print(conf_matrix2)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Greens', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set2')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set2:\")\n","print(classification_report(y2, y2_pred))"]},{"cell_type":"markdown","metadata":{"id":"zkS6m9ebZjA7"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"UVNI-v0eZjA7"},"source":["#### 4.2.3 - Questions"]},{"cell_type":"markdown","metadata":{"id":"AxPY6C5CZjA7"},"source":["1. Based on the observations across all three datasets, compare the performance of the model with PCA, trained in this section to: (4 marks)\n","    * Model trained in Section 1 using *all features*.\n","    * Model trained in Section 2 using *selected features*.\n","    * Model trained in Section 3 using *group selected features*\n","\n","2. Please provide potential explanations for the differences observed in models' performances across between section 1 and 4. (2 marks)"]},{"cell_type":"markdown","metadata":{"id":"ZXtb1wSmZjA7"},"source":["#### 4.2.3 - Answers\n","\n","1.1. The model with PCA in Section 4 outperforms the Section 1 model in terms of generalization, as PCA reduces dimensionality and removes redundant features, mitigating overfitting. While the Section 1 model may achieve higher accuracy on training data due to using all features, its performance on independent datasets suffers compared to the PCA model, which simplifies the feature space for better robustness.\n","\n","1.2. The Section 4 PCA model generally performs better than the Section 2 model because it identifies the most significant patterns across all features, while Section 2 relies on pre-selected features that might exclude critical information. PCA’s ability to combine features into principal components provides better generalization across diverse datasets, unlike the limited feature set in Section 2.\n","\n","1.3. The Section 4 PCA model offers comparable performance to the Section 3 model but with a more systematic approach to feature reduction. While the group-selected features in Section 3 rely on domain knowledge, PCA in Section 4 uses a data-driven approach to capture the most variability, leading to improved generalization, especially on datasets with differing feature distributions.\n","\n","2.The differences in performance between the models in Section 1 and Section 4 arise from how features are handled. In Section 1, the model uses all features, which may include redundant or irrelevant information, leading to overfitting and reduced generalization to independent datasets. In contrast, Section 4 applies PCA to reduce dimensionality by retaining only the most significant components, improving generalization by focusing on the underlying data structure and eliminating noise. This systematic feature reduction allows the PCA model to perform more robustly on unseen datasets."]},{"cell_type":"markdown","metadata":{"id":"zv2mRetSZjA7"},"source":["____"]},{"cell_type":"markdown","metadata":{"id":"9GHv0OaVZjA7"},"source":["## Section 5 - Classification Using *Pre Selected Features* Derived From Inertial Sensor Data (10 Marks)"]},{"cell_type":"markdown","metadata":{"id":"J2WFmHG8ZjA7"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"vzOUuChXZjA7"},"source":["Now, a second classifer is to be trained and tuned using grid search. The **selected features from section 2** must be used. You may choose from one of the algorithms discussed that class listed below.\n","1. Random Forest\n","1. K-Nearest Neighbors\n","1. Gradient Boosting Machines\n","1. Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"r8J91su8ZjA8"},"source":["____"]},{"cell_type":"markdown","metadata":{"id":"37KFfxU4ZjA8"},"source":["### 5.1 Grid Search with Cross Validation (6 Marks)"]},{"cell_type":"markdown","metadata":{"id":"S4crUvcSZjA8"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"BSFAWp51ZjA8"},"source":["#### 5.1.1 Extract *Selected* features and target from *cv_set*"]},{"cell_type":"markdown","metadata":{"id":"IXoxH1ZDZjA8"},"source":["**Questions**\n","1. Please list the ML model that you have selected . And explain *why you have chosen* this model? (2 marks)"]},{"cell_type":"markdown","metadata":{"id":"3xGwNU8aZjA8"},"source":["**Answer**\n","1. I selected Random Forest because it is a versatile, ensemble-based algorithm that works well with tabular data like the selected inertial sensor features. Random Forest effectively handles feature importance and interactions while being robust to noise and overfitting due to its bootstrap aggregation technique. It also requires less hyperparameter tuning compared to other complex algorithms like neural networks, making it efficient for this task. Additionally, its ability to provide feature importance insights adds interpretability, which is beneficial when working with pre-selected features from Section 2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUgxwDuPZjA8"},"outputs":[],"source":["# TODO - Load group selected features from cv_set to X\n","selected_features = ['ax_var', 'ay_var', 'az_sum', 'a_sum', 'gz_var']\n","X = cv_set[selected_features]\n","\n","# Display the first two rows of X to confirm\n","print(\"First two rows of selected features (X):\")\n","X.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yC0k2OZRZjA8"},"outputs":[],"source":["# TODO - Load motion_state column (target vector) into y\n","y = cv_set['motion_state']\n","\n","# Display the first few rows of y to confirm\n","print(\"\\nTarget vector (y):\")\n","y.head()"]},{"cell_type":"markdown","metadata":{"id":"wxqhq4gDZjA9"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"B8NdrR7iZjA9"},"source":["#### 5.1.2 - Configure and Perform Grid Search with Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"bWpTU86OZjA9"},"source":["Similar to the steps from **1.2.2** and perform model training and tuning using for the model selected. Perform the `train_test_split`, initiate a `Pipeline`, then define the *Parameters* for `GridSearchCV`. Finally, perfrom the grid search."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQBvCr3cZjA9"},"outputs":[],"source":["# TODO - Perform train/test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.30, random_state=42, stratify=y\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmIFqNZxZjA9"},"outputs":[],"source":["# Perform any missing imports\n","from sklearn.ensemble import RandomForestClassifier\n","# TODO - Initiate the pipeline\n","# (1 mark)\n","pipe = Pipeline([\n","    ('classifier', RandomForestClassifier(random_state=42))  # Random Forest as the classifier\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ccxrEaGZjA9"},"outputs":[],"source":["# TODO - Define the parameter grid for your model\n","# (3 marks)\n","param_grid = {\n","    'classifier__n_estimators': [50, 100],            # Reduce the number of trees\n","    'classifier__max_depth': [10, 20],               # Use fewer depth options\n","    'classifier__min_samples_split': [2, 5],         # Reduce split options\n","    'classifier__min_samples_leaf': [1, 2],          # Fewer leaf options\n","    'classifier__max_features': ['sqrt']             # Fix one option for max features\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dh9GftzJZjA9"},"outputs":[],"source":["# TODO - Perform the grid search\n","grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n","grid.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"Ejkeh7LoZjA9"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"RM-dTC9eZjA9"},"source":["#### 5.1.3 - Visualize Grid Search Results"]},{"cell_type":"markdown","metadata":{"id":"Ea_N5VEnZjA-"},"source":["Print the results from the grid search. The following outputs must be printed.\n","* Best parameter set\n","* Best cross-validation train score\n","* Best cross-validation test score\n","* Test set accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUaqefppZjA-"},"outputs":[],"source":["# TODO: Print the results from the grid search\n","\n","print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n","\n","best_train_score = max(grid.cv_results_['mean_train_score'])\n","print(\"Best cross-validation train score: {:.2f}\".format(best_train_score))\n","\n","best_validation_score = grid.best_score_\n","print(\"Best cross-validation validation score: {:.2f}\".format(best_validation_score))\n","\n","test_set_accuracy = grid.score(X_test, y_test)\n","print(\"Test-set score: {:.2f}\".format(test_set_accuracy))"]},{"cell_type":"markdown","metadata":{"id":"fKtWqvAkZjA-"},"source":["Display the confusion matrix and print the classification report for the trained model using the *test* split."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1F68j6qyZjA-"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report\n","# Predictions on the test set\n","y_pred = grid.best_estimator_.predict(X_test)\n","\n","# Confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(conf_matrix)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"m2okGbewZjA-"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"MDlGJyAdZjA-"},"source":["### 5.2 - Model Evaluation on Test Datasets 1 and 2 (4 Marks)"]},{"cell_type":"markdown","metadata":{"id":"tK-Ms0kBZjA-"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"M5Gnnu1dZjA-"},"source":["#### 5.2.1 - Extract selected features and targets from test datasets"]},{"cell_type":"markdown","metadata":{"id":"ij5OpLQFZjA-"},"source":["The model trained in 5.1, is to be tested on the independent datasets, that is test_set1 and test_set2.\n","Seperate the testsets into features and target vectors *(X1, y1)* and *(X2,y2)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0-BaRdXZjA-"},"outputs":[],"source":["# TODO - Load the target vector from test_set1 into y1, and the selected features from test_set1 into X1\n","selected_features = ['ax_var', 'ay_var', 'az_sum', 'a_sum', 'gz_var']\n","y1 = test_set1['motion_state']\n","X1 = test_set1[selected_features]\n","\n","# Display the first two rows of X1 to confirm\n","print(\"First two rows of selected features from test_set1 (X1):\")\n","X1.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tK8EnxorZjA-"},"outputs":[],"source":["# TODO - Load the target vector from test_set2 into y2, and the selected features from test_set2 into X2\n","y2 = test_set2['motion_state']\n","X2 = test_set2[selected_features]\n","\n","# Display the first two rows of X2 to confirm\n","print(\"\\nFirst two rows of selected features from test_set2 (X2):\")\n","X2.head(2)"]},{"cell_type":"markdown","metadata":{"id":"aYS-wCfDZjA_"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"Xh5ZbY27ZjA_"},"source":["#### 5.2.2 - Visualize Results"]},{"cell_type":"markdown","metadata":{"id":"M2UNmNDXZjA_"},"source":["For each test dataset, print the test set accuracy score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYyin4CfZjA_"},"outputs":[],"source":["# TODO - Print test score accuracies for test_set1 and test_set2\n","# Predict for test_set1 and calculate accuracy\n","y1_pred = grid.best_estimator_.predict(X1)\n","test_set1_score = accuracy_score(y1, y1_pred)\n","print(\"test_set1 score: {:.2f}\".format(test_set1_score))\n","\n","# Predict for test_set2 and calculate accuracy\n","y2_pred = grid.best_estimator_.predict(X2)\n","test_set2_score = accuracy_score(y2, y2_pred)\n","print(\"test_set2 score: {:.2f}\".format(test_set2_score))"]},{"cell_type":"markdown","metadata":{"id":"rJSpl4ERZjA_"},"source":["Display the confusion matrix and print the classification for test_set 1 *(X1, y1)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCMcaKiMZjA_"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X1, y1)\n","# Confusion matrix\n","conf_matrix1 = confusion_matrix(y1, y1_pred)\n","print(\"\\nConfusion Matrix for test_set1:\")\n","print(conf_matrix1)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix1, annot=True, fmt='d', cmap='Blues', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set1')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set1:\")\n","print(classification_report(y1, y1_pred))"]},{"cell_type":"markdown","metadata":{"id":"r34KP_sFZjA_"},"source":["Display the confusion matrix and print the classification for test_set 2 *(X2, y2)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0Nh8_h6ZjA_"},"outputs":[],"source":["# TODO - Display confusion matrix and print classification report for (X2, y2)\n","# Confusion matrix\n","conf_matrix2 = confusion_matrix(y2, y2_pred)\n","print(\"\\nConfusion Matrix for test_set2:\")\n","print(conf_matrix2)\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Greens', xticklabels=grid.classes_, yticklabels=grid.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for test_set2')\n","plt.show()\n","\n","# Classification report\n","print(\"\\nClassification Report for test_set2:\")\n","print(classification_report(y2, y2_pred))"]},{"cell_type":"markdown","metadata":{"id":"UTxeK8xHZjA_"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"QV1r_Nq6ZjA_"},"source":["#### 5.2.3 Question"]},{"cell_type":"markdown","metadata":{"id":"x6DygGfLZjA_"},"source":["1. For the model trainied in this section, compare its performance to the performance of SVC which was also trained with the same selected features (section 2). Which model fits better? Explain causes for any observed discrepancies? (4 marks)"]},{"cell_type":"markdown","metadata":{"id":"bnkmvkTcZjBA"},"source":["#### 5.2.3 - Answer\n","1. The SVC model from Section 2 outperforms the Random Forest model trained in Section 5 on both test datasets, achieving higher accuracy, precision, and recall, especially for the turn and straight classes. This suggests that SVC better captures the relationships between the selected features and the target classes. The discrepancies can be attributed to the strengths of SVC in handling small datasets and complex decision boundaries, which are likely important for the turn class. In contrast, Random Forest's ensemble approach may struggle with class imbalances and does not model decision boundaries as finely as SVC, leading to lower performance, particularly for minority classes like turn."]},{"cell_type":"markdown","metadata":{"id":"F2zCBvDKZjBA"},"source":["____"]}],"metadata":{"kernelspec":{"display_name":"PROJ682_ENV","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}